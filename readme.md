# Ticket Classification MLOps
## Project Structure

        ticket-classification-mlops/
        ‚îÇ
        ‚îú‚îÄ‚îÄ README.md                 # Global project documentation: architecture, setup, 
        ‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies for runtime (scikit-learn..)
        ‚îú‚îÄ‚îÄ pyproject.toml            # Project packaging + linting + formatting config (black,) 
        ‚îú‚îÄ‚îÄ .env                      # Environment variables (HF model name, DB paths, configs)
        ‚îú‚îÄ‚îÄ .gitignore                # Files ignored by git (models, data, env, cache)
        ‚îÇ
        ‚îú‚îÄ‚îÄ data/
        ‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Original untouched dataset (emails, tickets)
        ‚îÇ   ‚îú‚îÄ‚îÄ processed/            # Cleaned NLP-ready datasets used for training
        ‚îÇ   ‚îú‚îÄ‚îÄ interim/              # Temporary intermediate datasets between pipeline steps
        ‚îÇ   ‚îî‚îÄ‚îÄ reference/            # Baseline dataset used by Evidently for drift comparison
        ‚îÇ
        ‚îú‚îÄ‚îÄ notebooks/
        ‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb          # Exploratory Data Analysis: class distribution, text 
        ‚îÇ   ‚îú‚îÄ‚îÄ 02_embedding_tests.ipynb # Testing HuggingFace embedding models
        ‚îÇ   ‚îî‚îÄ‚îÄ 03_model_experiments.ipynb # Training experiments & model comparison
        ‚îÇ
        ‚îú‚îÄ‚îÄ src/
        ‚îÇ   ‚îú‚îÄ‚îÄ config/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py       # Global configs (model names, batch size, hyperparameters)
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ paths.py          # Centralized file paths (data dirs, model dirs, reports)
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ data/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_data.py      # Load CSV/JSON/email datasets
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py  # Pipeline preprocessing logic (merge fields, clean 
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_cleaning.py  # NLP cleaning: lowercase, stopwords, punctuation removal
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ features/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_generator.py # HuggingFace embedding generation
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_normalizer.py   # Vector normalization (L2 norm, scaling)
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chromadb_client.py     # ChromaDB connection, insert embeddings, search logic
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ models/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py         # Training classifier (scikit-learn)
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py      # Metrics: accuracy, precision, recall, F1
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ predict.py       # Prediction logic for new tickets
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ monitoring/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evidently_report.py # Generate Evidently drift/performance reports
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ drift_detection.py  # Compare reference vs current datasets
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_batch.py # Main orchestrator running full batch pipeline
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_preprocessing.py # Executes preprocessing step only
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_embeddings.py    # Generates embeddings & stores in ChromaDB
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_training.py      # Trains classification model
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_monitoring.py    # Runs Evidently monitoring & report generation
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îî‚îÄ‚îÄ utils/
        ‚îÇ       ‚îú‚îÄ‚îÄ logger.py       # Central logging config for pipeline execution
        ‚îÇ       ‚îî‚îÄ‚îÄ helpers.py      # Shared utilities (timers, validation, serialization)
        ‚îÇ
        ‚îú‚îÄ‚îÄ models/
        ‚îÇ   ‚îú‚îÄ‚îÄ trained/            # Final trained ML models (.pkl, .joblib)
        ‚îÇ   ‚îî‚îÄ‚îÄ artifacts/          # Training artifacts (scalers, encoders, metrics JSON)
        ‚îÇ
        ‚îú‚îÄ‚îÄ vector_db/
        ‚îÇ   ‚îî‚îÄ‚îÄ chroma_storage/     # Persistent storage of embeddings used by ChromaDB
        ‚îÇ
        ‚îú‚îÄ‚îÄ reports/
        ‚îÇ   ‚îú‚îÄ‚îÄ evidently/          # HTML reports generated by Evidently
        ‚îÇ   ‚îî‚îÄ‚îÄ metrics/            # Evaluation results, confusion matrices, logs
        ‚îÇ
        ‚îú‚îÄ‚îÄ docker/
        ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.pipeline   # Docker image for ML pipeline execution
        ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.monitoring # Docker image for monitoring components
        ‚îÇ   ‚îî‚îÄ‚îÄ entrypoint.sh         # Startup script executed when container launches
        ‚îÇ
        ‚îú‚îÄ‚îÄ k8s/
        ‚îÇ   ‚îú‚îÄ‚îÄ job-pipeline.yaml    # Kubernetes Job for batch pipeline execution
        ‚îÇ   ‚îú‚îÄ‚îÄ cronjob-pipeline.yaml# Scheduled retraining using CronJob
        ‚îÇ   ‚îî‚îÄ‚îÄ namespace.yaml       # Kubernetes namespace definition
        ‚îÇ
        ‚îú‚îÄ‚îÄ monitoring/
        ‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml   # Prometheus scraping config (cadvisor, node exporter)
        ‚îÇ   ‚îî‚îÄ‚îÄ grafana/
        ‚îÇ       ‚îî‚îÄ‚îÄ dashboards/      # Custom Grafana dashboards JSON configs
        ‚îÇ
        ‚îú‚îÄ‚îÄ ci/
        ‚îÇ   ‚îî‚îÄ‚îÄ github-actions/
        ‚îÇ       ‚îî‚îÄ‚îÄ ml-pipeline.yml  # CI/CD pipeline (lint ‚Üí test ‚Üí docker build ‚Üí push)
        ‚îÇ
        ‚îî‚îÄ‚îÄ scripts/
            ‚îú‚îÄ‚îÄ run_local_pipeline.sh # Run full pipeline locally for development/testing
            ‚îî‚îÄ‚îÄ build_images.sh       # Script to build Docker images quickly

## how to handle tags : assemble them in one column then make embedding



# Industrialisation d'un Pipeline NLP de Classification de Tickets Support avec MLOps

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Supported-326CE5.svg)](https://kubernetes.io/)
[![MLOps](https://img.shields.io/badge/MLOps-Enabled-green.svg)](https://ml-ops.org/)

## Auteur

**Yassine Ennaya** ‚Äì Cr√©√© le 07/02/2026

---

## Contexte du Projet

Ce projet a √©t√© r√©alis√© dans le cadre d'une mission en entreprise IT disposant d'un historique d'emails de support client.

Chaque ticket contient :
- **Champs textuels** : `subject`, `body`, `answer`
- **M√©tadonn√©es m√©tier** : priorit√©, type, queue, langue, etc.

### Objectif

Industrialiser un pipeline batch NLP permettant de :

1. ‚úÖ Traiter et comprendre le contenu des emails support
2. üß† G√©n√©rer des repr√©sentations s√©mantiques (embeddings) avec un mod√®le Hugging Face
3. üéØ Entra√Æner un mod√®le de classification supervis√©e pour pr√©dire le type de ticket
4. üíæ Stocker les embeddings dans une base vectorielle ChromaDB
5. üìä Surveiller la qualit√© du mod√®le et la d√©rive des donn√©es avec Evidently AI
6. üîç Superviser l'infrastructure avec Prometheus et Grafana

> **Note** : L'ensemble du projet est ex√©cut√© dans un environnement containeris√© (Docker & Kubernetes) **sans exposition d'API**.

---

## Structure du Projet

```
ticket-classification-mlops/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_data.py           # Chargement des donn√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaning.py            # Pr√©processing NLP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ encoding.py            # Encodage des variables cat√©gorielles
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedding_generator.py # G√©n√©ration et stockage des embeddings
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ vectors_store/
‚îÇ       ‚îî‚îÄ‚îÄ chromadb_client.py     # Client ChromaDB
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile                 # Containerisation du pipeline
‚îÇ
‚îú‚îÄ‚îÄ k8s/
‚îÇ   ‚îî‚îÄ‚îÄ *.yaml                     # Manifests Kubernetes pour Jobs / CronJobs
‚îÇ
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml         # Prometheus, Grafana, Node Exporter, cAdvisor
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## √âtapes du Pipeline

### 1Ô∏è‚É£ Analyse Exploratoire & Pr√©paration NLP

- Analyse des types de tickets, longueur des emails
- Fusion des champs textuels (`subject + body`)
- Nettoyage NLP :
  - Conversion en minuscules
  - Suppression de la ponctuation
  - Tokenisation
  - Suppression des stopwords selon la langue

### 2Ô∏è‚É£ G√©n√©ration d'Embeddings

- S√©lection d'un mod√®le pr√©-entra√Æn√© Hugging Face (`all-MiniLM-L6-v2`)
- Encodage des textes nettoy√©s en vecteurs
- Normalisation des vecteurs
- Stockage dans ChromaDB

### 3Ô∏è‚É£ Entra√Ænement du Mod√®le de Classification

- S√©paration train/test
- Entra√Ænement avec scikit-learn (ex: RandomForest ou Logistic Regression)
- √âvaluation : pr√©cision, recall, F1-score

### 4Ô∏è‚É£ Monitoring ML avec Evidently AI

- D√©finition d'un jeu de r√©f√©rence (baseline)
- Suivi de **data drift** et **prediction drift**
- G√©n√©ration de rapports HTML interactifs

### 5Ô∏è‚É£ Conteneurisation et Orchestration

- Dockerisation du pipeline NLP & ML
- D√©ploiement batch sur Kubernetes (Minikube)
- CI/CD avec GitHub Actions (lint + build Docker)

### 6Ô∏è‚É£ Monitoring Infrastructure avec Prometheus & Grafana

- **Node Exporter** : m√©triques CPU/RAM/disque
- **cAdvisor** : consommation des containers Docker
- Dashboards Grafana configur√©s pour visualisation

---

## üõ†Ô∏è D√©pendances Principales

- Python >= 3.10
- pandas, numpy, scikit-learn, nltk
- langchain_community.embeddings
- ChromaDB
- Docker & Kubernetes
- Prometheus & Grafana
- Evidently AI

---

## Instructions pour Ex√©cuter le Pipeline

### 1. Installer l'Environnement

```bash
python -m venv env
source env/bin/activate  # Sur Windows: env\Scripts\activate
pip install -r requirements.txt
```

### 2. Pr√©traitement des Donn√©es

```python
from src.data.load_data import load_data
from src.data.cleaning import cleaning
from src.data.encoding import encoding

df = load_data()
df_clean = cleaning(df)
df_encoded = encoding(df_clean)
```

### 3. G√©n√©ration et Stockage des Embeddings

```python
from src.features.embedding_generator import generate_embeddings

generate_embeddings(df_encoded)
```

### 4. Ex√©cution du Mod√®le de Classification

```python
from src.models.classifier import train_model, evaluate_model

model = train_model(X_train, y_train)
metrics = evaluate_model(model, X_test, y_test)
```

### 5. Monitoring ML

```python
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, ClassificationPreset

# G√©n√©ration de rapports Evidently AI pour data/prediction drift
report = Report(metrics=[DataDriftPreset(), ClassificationPreset()])
report.run(reference_data=reference_df, current_data=current_df)
report.save_html("drift_report.html")
```

### 6. Ex√©cution Containeris√©e

```bash
# Build Docker image
docker build -t ticket-nlp-pipeline ./docker

# D√©ploiement sur Kubernetes
kubectl apply -f k8s/
```

### 7. Monitoring Infrastructure

```bash
cd monitoring
docker-compose up -d
```

Acc√®s aux interfaces :
- **Prometheus** : http://localhost:9090
- **Grafana** : http://localhost:3000 (admin/admin)

---

## Livrables

- ‚úÖ Scripts de preprocessing NLP
- ‚úÖ Embeddings stock√©s dans ChromaDB
- ‚úÖ Mod√®le de classification entra√Æn√©
- ‚úÖ Rapports Evidently AI
- ‚úÖ Images Docker & manifests Kubernetes
- ‚úÖ Rapport technique final

---

## Crit√®res de Performance

| Crit√®re | Description |
|---------|-------------|
| **Qualit√© NLP** | Nettoyage efficace des textes, tokenisation adapt√©e |
| **Embeddings** | Coh√©rence des vecteurs et indexation ChromaDB optimale |
| **Classification** | Pr√©cision > 85%, F1-score √©quilibr√© |
| **Monitoring ML** | D√©tection proactive de drift avec Evidently AI |
| **Infrastructure** | Dashboards Prometheus/Grafana op√©rationnels |
| **Reproductibilit√©** | Pipeline compl√®tement automatis√© via Docker/Kubernetes |

---

## Remarques

- Le pipeline est con√ßu pour √™tre **batch**, non expos√© en API
- Toutes les √©tapes sont supervis√©es via MLOps pour garantir stabilit√© et maintenance continue
- L'orchestration Kubernetes permet une scalabilit√© horizontale
- Les m√©triques de monitoring permettent une d√©tection pr√©coce des anomalies

---


