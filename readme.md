# Ticket Classification MLOps
## Project Structure

        ticket-classification-mlops/
        │
        ├── README.md                 # Global project documentation: architecture, setup, 
        ├── requirements.txt          # Python dependencies for runtime (scikit-learn..)
        ├── pyproject.toml            # Project packaging + linting + formatting config (black,) 
        ├── .env                      # Environment variables (HF model name, DB paths, configs)
        ├── .gitignore                # Files ignored by git (models, data, env, cache)
        │
        ├── data/
        │   ├── raw/                  # Original untouched dataset (emails, tickets)
        │   ├── processed/            # Cleaned NLP-ready datasets used for training
        │   ├── interim/              # Temporary intermediate datasets between pipeline steps
        │   └── reference/            # Baseline dataset used by Evidently for drift comparison
        │
        ├── notebooks/
        │   ├── 01_eda.ipynb          # Exploratory Data Analysis: class distribution, text 
        │   ├── 02_embedding_tests.ipynb # Testing HuggingFace embedding models
        │   └── 03_model_experiments.ipynb # Training experiments & model comparison
        │
        ├── src/
        │   ├── config/
        │   │   ├── settings.py       # Global configs (model names, batch size, hyperparameters)
        │   │   └── paths.py          # Centralized file paths (data dirs, model dirs, reports)
        │   │
        │   ├── data/
        │   │   ├── load_data.py      # Load CSV/JSON/email datasets
        │   │   ├── preprocessing.py  # Pipeline preprocessing logic (merge fields, clean 
        │   │   └── text_cleaning.py  # NLP cleaning: lowercase, stopwords, punctuation removal
        │   │
        │   ├── features/
        │   │   ├── embedding_generator.py # HuggingFace embedding generation
        │   │   └── vector_normalizer.py   # Vector normalization (L2 norm, scaling)
        │   │
        │   ├── vectorstore/
        │   │   └── chromadb_client.py     # ChromaDB connection, insert embeddings, search logic
        │   │
        │   ├── models/
        │   │   ├── train.py         # Training classifier (scikit-learn)
        │   │   ├── evaluate.py      # Metrics: accuracy, precision, recall, F1
        │   │   └── predict.py       # Prediction logic for new tickets
        │   │
        │   ├── monitoring/
        │   │   ├── evidently_report.py # Generate Evidently drift/performance reports
        │   │   └── drift_detection.py  # Compare reference vs current datasets
        │   │
        │   ├── pipeline/
        │   │   ├── pipeline_batch.py # Main orchestrator running full batch pipeline
        │   │   ├── run_preprocessing.py # Executes preprocessing step only
        │   │   ├── run_embeddings.py    # Generates embeddings & stores in ChromaDB
        │   │   ├── run_training.py      # Trains classification model
        │   │   └── run_monitoring.py    # Runs Evidently monitoring & report generation
        │   │
        │   └── utils/
        │       ├── logger.py       # Central logging config for pipeline execution
        │       └── helpers.py      # Shared utilities (timers, validation, serialization)
        │
        ├── models/
        │   ├── trained/            # Final trained ML models (.pkl, .joblib)
        │   └── artifacts/          # Training artifacts (scalers, encoders, metrics JSON)
        │
        ├── vector_db/
        │   └── chroma_storage/     # Persistent storage of embeddings used by ChromaDB
        │
        ├── reports/
        │   ├── evidently/          # HTML reports generated by Evidently
        │   └── metrics/            # Evaluation results, confusion matrices, logs
        │
        ├── docker/
        │   ├── Dockerfile.pipeline   # Docker image for ML pipeline execution
        │   ├── Dockerfile.monitoring # Docker image for monitoring components
        │   └── entrypoint.sh         # Startup script executed when container launches
        │
        ├── k8s/
        │   ├── job-pipeline.yaml    # Kubernetes Job for batch pipeline execution
        │   ├── cronjob-pipeline.yaml# Scheduled retraining using CronJob
        │   └── namespace.yaml       # Kubernetes namespace definition
        │
        ├── monitoring/
        │   ├── prometheus/
        │   │   └── prometheus.yml   # Prometheus scraping config (cadvisor, node exporter)
        │   └── grafana/
        │       └── dashboards/      # Custom Grafana dashboards JSON configs
        │
        ├── ci/
        │   └── github-actions/
        │       └── ml-pipeline.yml  # CI/CD pipeline (lint → test → docker build → push)
        │
        └── scripts/
            ├── run_local_pipeline.sh # Run full pipeline locally for development/testing
            └── build_images.sh       # Script to build Docker images quickly

## how to handle tags : assemble them in one column then make embedding